{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor, positive, negative, alpha):\n",
    "    \"\"\"Calculate the triplet loss according to the FaceNet paper\n",
    "    \n",
    "    Args:\n",
    "      anchor: the embeddings for the anchor images.\n",
    "      positive: the embeddings for the positive images.\n",
    "      negative: the embeddings for the negative images.\n",
    "  \n",
    "    Returns:\n",
    "      the triplet loss according to the FaceNet paper as a float tensor.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('triplet_loss'):\n",
    "        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n",
    "        \n",
    "        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n",
    "        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n",
    "      \n",
    "    return loss\n",
    "  \n",
    "def center_loss(features, label, alfa, nrof_classes):\n",
    "    \"\"\"Center loss based on the paper \"A Discriminative Feature Learning Approach for Deep Face Recognition\"\n",
    "       (http://ydwen.github.io/papers/WenECCV16.pdf)\n",
    "    \"\"\"\n",
    "    nrof_features = features.get_shape()[1]\n",
    "    centers = tf.get_variable('centers', [nrof_classes, nrof_features], dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(0), trainable=False)\n",
    "    label = tf.reshape(label, [-1])\n",
    "    centers_batch = tf.gather(centers, label)\n",
    "    diff = (1 - alfa) * (centers_batch - features)\n",
    "    centers = tf.scatter_sub(centers, label, diff)\n",
    "    with tf.control_dependencies([centers]):\n",
    "        loss = tf.reduce_mean(tf.square(features - centers_batch))\n",
    "    return loss, centers\n",
    "\n",
    "def get_image_paths_and_labels(dataset):\n",
    "    image_paths_flat = []\n",
    "    labels_flat = []\n",
    "    for i in range(len(dataset)):\n",
    "        image_paths_flat += dataset[i].image_paths\n",
    "        labels_flat += [i] * len(dataset[i].image_paths)\n",
    "        print ( image_paths_flat,labels_flat)\n",
    "    return image_paths_flat, labels_flat\n",
    "\n",
    "def shuffle_examples(image_paths, labels):\n",
    "    shuffle_list = list(zip(image_paths, labels))\n",
    "    random.shuffle(shuffle_list)\n",
    "    image_paths_shuff, labels_shuff = zip(*shuffle_list)\n",
    "    return image_paths_shuff, labels_shuff\n",
    "\n",
    "def random_rotate_image(image):\n",
    "    angle = np.random.uniform(low=-10.0, high=10.0)\n",
    "    return misc.imrotate(image, angle, 'bicubic')\n",
    "  \n",
    "# 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\n",
    "RANDOM_ROTATE = 1\n",
    "RANDOM_CROP = 2\n",
    "RANDOM_FLIP = 4\n",
    "FIXED_STANDARDIZATION = 8\n",
    "FLIP = 16\n",
    "def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\n",
    "    images_and_labels_list = []\n",
    "    for _ in range(nrof_preprocess_threads):\n",
    "        filenames, label, control = input_queue.dequeue()\n",
    "        images = []\n",
    "        for filename in tf.unstack(filenames):\n",
    "            file_contents = tf.read_file(filename)\n",
    "            image = tf.image.decode_image(file_contents, 3)\n",
    "            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n",
    "                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n",
    "                            lambda:tf.identity(image))\n",
    "            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n",
    "                            lambda:tf.random_crop(image, image_size + (3,)), \n",
    "                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n",
    "            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n",
    "                            lambda:tf.image.random_flip_left_right(image),\n",
    "                            lambda:tf.identity(image))\n",
    "            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n",
    "                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n",
    "                            lambda:tf.image.per_image_standardization(image))\n",
    "            image = tf.cond(get_control_flag(control[0], FLIP),\n",
    "                            lambda:tf.image.flip_left_right(image),\n",
    "                            lambda:tf.identity(image))\n",
    "            #pylint: disable=no-member\n",
    "            image.set_shape(image_size + (3,))\n",
    "            images.append(image)\n",
    "        images_and_labels_list.append([images, label])\n",
    "\n",
    "    image_batch, label_batch = tf.train.batch_join(\n",
    "        images_and_labels_list, batch_size=batch_size_placeholder, \n",
    "        shapes=[image_size + (3,), ()], enqueue_many=True,\n",
    "        capacity=4 * nrof_preprocess_threads * 100,\n",
    "        allow_smaller_final_batch=True)\n",
    "    \n",
    "    return image_batch, label_batch\n",
    "\n",
    "def get_control_flag(control, field):\n",
    "    return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n",
    "  \n",
    "def _add_loss_summaries(total_loss):\n",
    "    \"\"\"Add summaries for losses.\n",
    "  \n",
    "    Generates moving average for all losses and associated summaries for\n",
    "    visualizing the performance of the network.\n",
    "  \n",
    "    Args:\n",
    "      total_loss: Total loss from loss().\n",
    "    Returns:\n",
    "      loss_averages_op: op for generating moving averages of losses.\n",
    "    \"\"\"\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "  \n",
    "    # Attach a scalar summmary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "        tf.summary.scalar(l.op.name +' (raw)', l)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "  \n",
    "    return loss_averages_op\n",
    "\n",
    "def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        if optimizer=='ADAGRAD':\n",
    "            opt = tf.train.AdagradOptimizer(learning_rate)\n",
    "        elif optimizer=='ADADELTA':\n",
    "            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n",
    "        elif optimizer=='ADAM':\n",
    "            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n",
    "        elif optimizer=='RMSPROP':\n",
    "            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n",
    "        elif optimizer=='MOM':\n",
    "            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimization algorithm')\n",
    "    \n",
    "        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n",
    "        \n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "  \n",
    "    # Add histograms for trainable variables.\n",
    "    if log_histograms:\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "   \n",
    "    # Add histograms for gradients.\n",
    "    if log_histograms:\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "  \n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        moving_average_decay, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "  \n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "  \n",
    "    return train_op\n",
    "\n",
    "def prewhiten(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n",
    "    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n",
    "    return y  \n",
    "\n",
    "def crop(image, random_crop, image_size):\n",
    "    if image.shape[1]>image_size:\n",
    "        sz1 = int(image.shape[1]//2)\n",
    "        sz2 = int(image_size//2)\n",
    "        if random_crop:\n",
    "            diff = sz1-sz2\n",
    "            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n",
    "        else:\n",
    "            (h, v) = (0,0)\n",
    "        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n",
    "    return image\n",
    "  \n",
    "def flip(image, random_flip):\n",
    "    if random_flip and np.random.choice([True, False]):\n",
    "        image = np.fliplr(image)\n",
    "    return image\n",
    "\n",
    "def to_rgb(img):\n",
    "    w, h = img.shape\n",
    "    ret = np.empty((w, h, 3), dtype=np.uint8)\n",
    "    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n",
    "    return ret\n",
    "  \n",
    "def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\n",
    "    nrof_samples = len(image_paths)\n",
    "    images = np.zeros((nrof_samples, image_size, image_size, 3))\n",
    "    for i in range(nrof_samples):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        if img.ndim == 2:\n",
    "            img = to_rgb(img)\n",
    "        if do_prewhiten:\n",
    "            img = prewhiten(img)\n",
    "        img = crop(img, do_random_crop, image_size)\n",
    "        img = flip(img, do_random_flip)\n",
    "        images[i,:,:,:] = img\n",
    "    return images\n",
    "\n",
    "def get_label_batch(label_data, batch_size, batch_index):\n",
    "    nrof_examples = np.size(label_data, 0)\n",
    "    j = batch_index*batch_size % nrof_examples\n",
    "    if j+batch_size<=nrof_examples:\n",
    "        batch = label_data[j:j+batch_size]\n",
    "    else:\n",
    "        x1 = label_data[j:nrof_examples]\n",
    "        x2 = label_data[0:nrof_examples-j]\n",
    "        batch = np.vstack([x1,x2])\n",
    "    batch_int = batch.astype(np.int64)\n",
    "    return batch_int\n",
    "\n",
    "def get_batch(image_data, batch_size, batch_index):\n",
    "    nrof_examples = np.size(image_data, 0)\n",
    "    j = batch_index*batch_size % nrof_examples\n",
    "    if j+batch_size<=nrof_examples:\n",
    "        batch = image_data[j:j+batch_size,:,:,:]\n",
    "    else:\n",
    "        x1 = image_data[j:nrof_examples,:,:,:]\n",
    "        x2 = image_data[0:nrof_examples-j,:,:,:]\n",
    "        batch = np.vstack([x1,x2])\n",
    "    batch_float = batch.astype(np.float32)\n",
    "    return batch_float\n",
    "\n",
    "def get_triplet_batch(triplets, batch_index, batch_size):\n",
    "    ax, px, nx = triplets\n",
    "    a = get_batch(ax, int(batch_size/3), batch_index)\n",
    "    p = get_batch(px, int(batch_size/3), batch_index)\n",
    "    n = get_batch(nx, int(batch_size/3), batch_index)\n",
    "    batch = np.vstack([a, p, n])\n",
    "    return batch\n",
    "\n",
    "def get_learning_rate_from_file(filename, epoch):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split('#', 1)[0]\n",
    "            if line:\n",
    "                par = line.strip().split(':')\n",
    "                e = int(par[0])\n",
    "                if par[1]=='-':\n",
    "                    lr = -1\n",
    "                else:\n",
    "                    lr = float(par[1])\n",
    "                if e <= epoch:\n",
    "                    learning_rate = lr\n",
    "                else:\n",
    "                    return learning_rate\n",
    "\n",
    "class ImageClass():\n",
    "    \"Stores the paths to images for a given class\"\n",
    "    def __init__(self, name, image_paths):\n",
    "        self.name = name\n",
    "        self.image_paths = image_paths\n",
    "  \n",
    "    def __str__(self):\n",
    "        return self.name + ', ' + str(len(self.image_paths)) + ' images'\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "  \n",
    "def get_dataset(path, has_class_directories=True):\n",
    "    dataset = []\n",
    "    path_exp = os.path.expanduser(path)\n",
    "    print ( path)\n",
    "    classes = [path for path in os.listdir(path_exp) \\\n",
    "                    if os.path.isdir(os.path.join(path_exp, path))]\n",
    "    classes.sort()\n",
    "    nrof_classes = len(classes)\n",
    "    for i in range(nrof_classes):\n",
    "        class_name = classes[i]\n",
    "        facedir = os.path.join(path_exp, class_name)\n",
    "        image_paths = get_image_paths(facedir)\n",
    "        dataset.append(ImageClass(class_name, image_paths))\n",
    "    print ( dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_image_paths(facedir):\n",
    "    image_paths = []\n",
    "    if os.path.isdir(facedir):\n",
    "        images = os.listdir(facedir)\n",
    "        image_paths = [os.path.join(facedir,img) for img in images]\n",
    "    return image_paths\n",
    "  \n",
    "def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n",
    "    if mode=='SPLIT_CLASSES':\n",
    "        nrof_classes = len(dataset)\n",
    "        class_indices = np.arange(nrof_classes)\n",
    "        np.random.shuffle(class_indices)\n",
    "        split = int(round(nrof_classes*(1-split_ratio)))\n",
    "        train_set = [dataset[i] for i in class_indices[0:split]]\n",
    "        test_set = [dataset[i] for i in class_indices[split:-1]]\n",
    "    elif mode=='SPLIT_IMAGES':\n",
    "        train_set = []\n",
    "        test_set = []\n",
    "        for cls in dataset:\n",
    "            paths = cls.image_paths\n",
    "            np.random.shuffle(paths)\n",
    "            nrof_images_in_class = len(paths)\n",
    "            split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n",
    "            if split==nrof_images_in_class:\n",
    "                split = nrof_images_in_class-1\n",
    "            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n",
    "                train_set.append(ImageClass(cls.name, paths[:split]))\n",
    "                test_set.append(ImageClass(cls.name, paths[split:]))\n",
    "    else:\n",
    "        raise ValueError('Invalid train/test split mode \"%s\"' % mode)\n",
    "    return train_set, test_set\n",
    "\n",
    "def load_model(model, input_map=None):\n",
    "    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n",
    "    #  or if it is a protobuf file with a frozen graph\n",
    "    model_exp = os.path.expanduser(model)\n",
    "    if (os.path.isfile(model_exp)):\n",
    "        print('Model filename: %s' % model_exp)\n",
    "        with tf.gfile.FastGFile(model_exp,'rb') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            tf.import_graph_def(graph_def, input_map=input_map, name='')\n",
    "    else:\n",
    "        print('Model directory: %s' % model_exp)\n",
    "        meta_file, ckpt_file = get_model_filenames(model_exp)\n",
    "        \n",
    "        print('Metagraph file: %s' % meta_file)\n",
    "        print('Checkpoint file: %s' % ckpt_file)\n",
    "      \n",
    "        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n",
    "        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n",
    "    \n",
    "def get_model_filenames(model_dir):\n",
    "    files = os.listdir(model_dir)\n",
    "    meta_files = [s for s in files if s.endswith('.meta')]\n",
    "    if len(meta_files)==0:\n",
    "        raise ValueError('No meta file found in the model directory (%s)' % model_dir)\n",
    "    elif len(meta_files)>1:\n",
    "        raise ValueError('There should not be more than one meta file in the model directory (%s)' % model_dir)\n",
    "    meta_file = meta_files[0]\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        return meta_file, ckpt_file\n",
    "\n",
    "    meta_files = [s for s in files if '.ckpt' in s]\n",
    "    max_step = -1\n",
    "    for f in files:\n",
    "        step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
    "        if step_str is not None and len(step_str.groups())>=2:\n",
    "            step = int(step_str.groups()[1])\n",
    "            if step > max_step:\n",
    "                max_step = step\n",
    "                ckpt_file = step_str.groups()[0]\n",
    "    return meta_file, ckpt_file\n",
    "  \n",
    "def distance(embeddings1, embeddings2, distance_metric=0):\n",
    "    if distance_metric==0:\n",
    "        # Euclidian distance\n",
    "        diff = np.subtract(embeddings1, embeddings2)\n",
    "        dist = np.sum(np.square(diff),1)\n",
    "    elif distance_metric==1:\n",
    "        # Distance based on cosine similarity\n",
    "        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n",
    "        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n",
    "        similarity = dot / norm\n",
    "        dist = np.arccos(similarity) / math.pi\n",
    "    else:\n",
    "        raise 'Undefined distance metric %d' % distance_metric \n",
    "        \n",
    "    return dist\n",
    "\n",
    "def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    assert(embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert(embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "    \n",
    "    tprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_folds,nrof_thresholds))\n",
    "    accuracy = np.zeros((nrof_folds))\n",
    "    \n",
    "    indices = np.arange(nrof_pairs)\n",
    "    \n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        if subtract_mean:\n",
    "            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n",
    "        else:\n",
    "          mean = 0.0\n",
    "        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n",
    "        \n",
    "        # Find the best threshold for the fold\n",
    "        acc_train = np.zeros((nrof_thresholds))\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n",
    "        best_threshold_index = np.argmax(acc_train)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n",
    "        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n",
    "          \n",
    "        tpr = np.mean(tprs,0)\n",
    "        fpr = np.mean(fprs,0)\n",
    "    return tpr, fpr, accuracy\n",
    "\n",
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "  \n",
    "    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n",
    "    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n",
    "    acc = float(tp+tn)/dist.size\n",
    "    return tpr, fpr, acc\n",
    "\n",
    "\n",
    "  \n",
    "def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    assert(embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert(embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n",
    "    \n",
    "    val = np.zeros(nrof_folds)\n",
    "    far = np.zeros(nrof_folds)\n",
    "    \n",
    "    indices = np.arange(nrof_pairs)\n",
    "    \n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        if subtract_mean:\n",
    "            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n",
    "        else:\n",
    "          mean = 0.0\n",
    "        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n",
    "      \n",
    "        # Find the threshold that gives FAR = far_target\n",
    "        far_train = np.zeros(nrof_thresholds)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n",
    "        if np.max(far_train)>=far_target:\n",
    "            f = interpolate.interp1d(far_train, thresholds, kind='slinear')\n",
    "            threshold = f(far_target)\n",
    "        else:\n",
    "            threshold = 0.0\n",
    "    \n",
    "        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n",
    "  \n",
    "    val_mean = np.mean(val)\n",
    "    far_mean = np.mean(far)\n",
    "    val_std = np.std(val)\n",
    "    return val_mean, val_std, far_mean\n",
    "\n",
    "\n",
    "def calculate_val_far(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    n_same = np.sum(actual_issame)\n",
    "    n_diff = np.sum(np.logical_not(actual_issame))\n",
    "    val = float(true_accept) / float(n_same)\n",
    "    far = float(false_accept) / float(n_diff)\n",
    "    return val, far\n",
    "\n",
    "def store_revision_info(src_path, output_dir, arg_string):\n",
    "    try:\n",
    "        # Get git hash\n",
    "        cmd = ['git', 'rev-parse', 'HEAD']\n",
    "        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n",
    "        (stdout, _) = gitproc.communicate()\n",
    "        git_hash = stdout.strip()\n",
    "    except OSError as e:\n",
    "        git_hash = ' '.join(cmd) + ': ' +  e.strerror\n",
    "  \n",
    "    try:\n",
    "        # Get local changes\n",
    "        cmd = ['git', 'diff', 'HEAD']\n",
    "        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n",
    "        (stdout, _) = gitproc.communicate()\n",
    "        git_diff = stdout.strip()\n",
    "    except OSError as e:\n",
    "        git_diff = ' '.join(cmd) + ': ' +  e.strerror\n",
    "    \n",
    "    # Store a text file in the log directory\n",
    "    rev_info_filename = os.path.join(output_dir, 'revision_info.txt')\n",
    "    with open(rev_info_filename, \"w\") as text_file:\n",
    "        text_file.write('arguments: %s\\n--------------------\\n' % arg_string)\n",
    "        text_file.write('tensorflow version: %s\\n--------------------\\n' % tf.__version__)  # @UndefinedVariable\n",
    "        text_file.write('git hash: %s\\n--------------------\\n' % git_hash)\n",
    "        text_file.write('%s' % git_diff)\n",
    "\n",
    "def list_variables(filename):\n",
    "    reader = training.NewCheckpointReader(filename)\n",
    "    variable_map = reader.get_variable_to_shape_map()\n",
    "    names = sorted(variable_map.keys())\n",
    "    return names\n",
    "\n",
    "def put_images_on_grid(images, shape=(16,8)):\n",
    "    nrof_images = images.shape[0]\n",
    "    img_size = images.shape[1]\n",
    "    bw = 3\n",
    "    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n",
    "    for i in range(shape[1]):\n",
    "        x_start = i*(img_size+bw)+bw\n",
    "        for j in range(shape[0]):\n",
    "            img_index = i*shape[0]+j\n",
    "            if img_index>=nrof_images:\n",
    "                break\n",
    "            y_start = j*(img_size+bw)+bw\n",
    "            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n",
    "        if img_index>=nrof_images:\n",
    "            break\n",
    "    return img\n",
    "\n",
    "def write_arguments_to_file(args, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for key, value in iteritems(vars(args)):\n",
    "            f.write('%s: %s\\n' % (key, str(value)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'detect_face'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-74555e7715ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfacenet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdetect_face\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'detect_face'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from scipy import misc\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import facenet\n",
    "import detect_face\n",
    "import random\n",
    "from time import sleep\n",
    "import cv2\n",
    "import matplotlib as pl\n",
    "# def get_dataset(path, has_class_directories=True):\n",
    "#     dataset = []\n",
    "#     path_exp = os.path.expanduser(path)\n",
    "#     classes = [path for path in os.listdir(path_exp) \\\n",
    "#                     if os.path.isdir(os.path.join(path_exp, path))]\n",
    "#     classes.sort()\n",
    "#     nrof_classes = len(classes)\n",
    "#     for i in range(nrof_classes):\n",
    "#         class_name = classes[i]\n",
    "#         facedir = os.path.join(path_exp, class_name)\n",
    "#         image_paths = get_image_paths(facedir)\n",
    "#         dataset.append(ImageClass(class_name, image_paths))\n",
    "  \n",
    "#     return dataset\n",
    "output_dir_path = '/Users/Mohamed/tensorflow/face_detection_output/'\n",
    "\n",
    "output_dir = os.path.expanduser(output_dir_path)\n",
    "if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "        \n",
    "path='/Users/Mohamed/tensorflow/new folder/'\n",
    "path='New folder'\n",
    "\n",
    "dataset = []\n",
    "path_exp = os.path.expanduser(path)\n",
    "listt=os.listdir(path_exp)\n",
    "print( listt)\n",
    "classes = [path for path in os.listdir(path_exp) \n",
    "                    ]\n",
    "gg=os.path.isdir(os.path.join(path_exp, path))\n",
    "\n",
    "classes.sort()\n",
    "nrof_classes = len(classes)\n",
    "print (nrof_classes )\n",
    "for i in range(nrof_classes):\n",
    "        class_name = classes[i]\n",
    "        facedir = os.path.join(path_exp, class_name)\n",
    "        image_paths = get_image_paths(facedir)\n",
    "        dataset.append(ImageClass(class_name, image_paths))\n",
    "  \n",
    "print( dataset)\n",
    "# # os.path.split(os.path.realpath('Users\\\\El Holandy\\\\AppData\\\\Roaming\\\\SPB_Data\\\\New folder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
